# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/dataops95/ml-zoomcamp-homeworks-2025/blob/main/capstone1/notebook.ipynb
"""

# ============================================================================
# CELL 1: Import Libraries
# ============================================================================
"""
Heart Disease Prediction - Comprehensive Analysis
Dataset: heart.csv (1,025 patient records)
Date: 2026-01-05

This notebook performs end-to-end machine learning pipeline for predicting
heart disease based on medical indicators.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score,
    precision_recall_curve, average_precision_score
)
from sklearn.dummy import DummyClassifier

import xgboost as xgb
import joblib

# Settings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("‚úÖ All libraries imported successfully!")
print(f"üì¶ Pandas version: {pd.__version__}")
print(f"üì¶ NumPy version: {np.__version__}")

# ============================================================================
# CELL 2: Load Dataset
# ============================================================================
"""
Load the Heart Disease dataset
Dataset: heart.csv (1,025 patient records, 14 columns)
Source: https://github.com/dataops95/ml-zoomcamp-homeworks-2025/blob/main/capstone1/data/heart.csv
"""

# Load from local file
df = pd.read_csv('data/heart.csv')

print("‚úÖ Dataset loaded successfully!")
print(f"\nüìä Dataset shape: {df.shape}")
print(f"   Number of samples: {df.shape[0]}")
print(f"   Number of features: {df.shape[1]}")

# Display column names
print(f"\nüìã Column names:")
print(df.columns.tolist())

# ============================================================================
# CELL 3: Initial Data Exploration
# ============================================================================
"""
Quick overview of the dataset structure
"""

print("=" * 80)
print("DATASET OVERVIEW")
print("=" * 80)

# First rows
print("\nüìä First 10 rows:")
display(df.head(10))

print("\nüìä Last 5 rows:")
display(df.tail())

# Random sample
print("\nüìä Random sample (5 rows):")
display(df.sample(5, random_state=42))

# Data types and non-null counts
print("\nüìã Dataset Info:")
df.info()

# ============================================================================
# CELL 4: Feature Description
# ============================================================================
"""
Understanding each feature in the dataset
"""

feature_descriptions = {
    'age': 'Age in years (29-77)',
    'sex': 'Sex (1 = male; 0 = female)',
    'cp': 'Chest pain type (0=typical angina, 1=atypical angina, 2=non-anginal, 3=asymptomatic)',
    'trestbps': 'Resting blood pressure in mm Hg (on admission to hospital)',
    'chol': 'Serum cholesterol in mg/dl',
    'fbs': 'Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)',
    'restecg': 'Resting electrocardiographic results (0=normal, 1=ST-T abnormality, 2=LV hypertrophy)',
    'thalach': 'Maximum heart rate achieved (bpm)',
    'exang': 'Exercise induced angina (1 = yes; 0 = no)',
    'oldpeak': 'ST depression induced by exercise relative to rest',
    'slope': 'Slope of peak exercise ST segment (0=upsloping, 1=flat, 2=downsloping)',
    'ca': 'Number of major vessels (0-4) colored by fluoroscopy',
    'thal': 'Thalassemia (0=unknown, 1=normal, 2=fixed defect, 3=reversible defect)',
    'target': 'Heart disease diagnosis (1 = disease; 0 = no disease)'
}

print("=" * 80)
print("FEATURE DESCRIPTIONS")
print("=" * 80)
for feature, description in feature_descriptions.items():
    print(f"‚Ä¢ {feature:12s}: {description}")

# ============================================================================
# CELL 5: Statistical Summary
# ============================================================================
"""
Statistical summary of numerical features
"""

print("=" * 80)
print("STATISTICAL SUMMARY")
print("=" * 80)

# Descriptive statistics
print("\nüìà Numerical Features Statistics:")
display(df.describe().T.style.background_gradient(cmap='RdYlGn', axis=0))

print("\nüìä Categorical Features Value Counts:")
categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']

for col in categorical_cols:
    if col in df.columns:
        print(f"\n{col.upper()}:")
        value_counts = df[col].value_counts().sort_index()
        for val, count in value_counts.items():
            pct = count / len(df) * 100
            print(f"  {val}: {count:4d} ({pct:5.1f}%)")

# ============================================================================
# CELL 6: Missing Values Analysis
# ============================================================================
"""
Check for missing values and data quality issues
"""

print("=" * 80)
print("MISSING VALUES ANALYSIS")
print("=" * 80)

# Missing values count
missing_values = df.isnull().sum()
missing_percent = (missing_values / len(df)) * 100

missing_df = pd.DataFrame({
    'Missing_Count': missing_values,
    'Missing_Percent': missing_percent
}).sort_values('Missing_Count', ascending=False)

print("\nüîç Missing Values Summary:")
if missing_df['Missing_Count'].sum() == 0:
    print("‚úÖ No missing values found! Data quality is excellent.")
else:
    print(missing_df[missing_df['Missing_Count'] > 0])
    print(f"\n‚ö†Ô∏è Total missing values: {missing_df['Missing_Count'].sum()}")

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nüîç Duplicate rows: {duplicates}")

if duplicates > 0:
    print(f"‚ö†Ô∏è Found {duplicates} duplicate rows.")
    print("First few duplicates:")
    display(df[df.duplicated(keep=False)].head(10))

    # Remove duplicates
    df = df.drop_duplicates()
    print(f"‚úÖ Duplicates removed. New shape: {df.shape}")
else:
    print("‚úÖ No duplicates detected!")

# Data quality checks
print(f"\nüìä Data Quality Checks:")
print(f"   Age range: {df['age'].min()}-{df['age'].max()} years")
print(f"   Blood pressure range: {df['trestbps'].min()}-{df['trestbps'].max()} mm Hg")
print(f"   Cholesterol range: {df['chol'].min()}-{df['chol'].max()} mg/dl")
print(f"   Max heart rate range: {df['thalach'].min()}-{df['thalach'].max()} bpm")

# Check for outliers/invalid values
invalid_chol = (df['chol'] == 0).sum()
invalid_bp = (df['trestbps'] == 0).sum()

if invalid_chol > 0 or invalid_bp > 0:
    print(f"\n‚ö†Ô∏è Data quality issues:")
    if invalid_chol > 0:
        print(f"   Cholesterol = 0: {invalid_chol} cases (measurement error?)")
    if invalid_bp > 0:
        print(f"   Blood pressure = 0: {invalid_bp} cases (measurement error?)")
else:
    print(f"\n‚úÖ No obvious data quality issues detected")

# ============================================================================
# CELL 7: Target Variable Analysis
# ============================================================================
"""
Analyze the distribution of the target variable (class balance)
"""

print("=" * 80)
print("TARGET VARIABLE ANALYSIS")
print("=" * 80)

# Class distribution
target_counts = df['target'].value_counts().sort_index()
target_percent = (target_counts / len(df)) * 100

print("\nüéØ Target Variable Distribution:")
print(f"   No Disease (0): {target_counts[0]} samples ({target_percent[0]:.1f}%)")
print(f"   Disease (1):    {target_counts[1]} samples ({target_percent[1]:.1f}%)")

# Calculate imbalance ratio
imbalance_ratio = target_counts[0] / target_counts[1] if target_counts[1] > target_counts[0] else target_counts[1] / target_counts[0]
print(f"\nüìä Class Balance:")
print(f"   Ratio (min/max): {imbalance_ratio:.2f}")

if imbalance_ratio > 0.8:
    print(f"   ‚úÖ Dataset is well-balanced! (ratio > 0.8)")
elif imbalance_ratio > 0.5:
    print(f"   ‚ö†Ô∏è Minor imbalance. Consider stratified sampling.")
else:
    print(f"   ‚ö†Ô∏è Significant imbalance! Consider SMOTE or class weights.")

# Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Count plot
sns.countplot(x='target', data=df, ax=axes[0], palette=['lightgreen', 'lightcoral'])
axes[0].set_title('Distribution of Target Variable', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Target (0 = No Disease, 1 = Disease)', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
axes[0].set_xticklabels(['No Disease', 'Disease'])

# Add counts on bars
for container in axes[0].containers:
    axes[0].bar_label(container, fontsize=11, fontweight='bold')

# Pie chart
colors = ['#90EE90', '#FFB6C1']
axes[1].pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%',
            colors=colors, startangle=90, textprops={'fontsize': 12}, explode=(0.05, 0.05))
axes[1].set_title('Target Variable Proportion', fontsize=14, fontweight='bold')

# Percentage bar chart
axes[2].barh(['Disease', 'No Disease'], [target_percent[1], target_percent[0]],
            color=['lightcoral', 'lightgreen'], edgecolor='black', alpha=0.7)
axes[2].set_xlabel('Percentage (%)', fontsize=12)
axes[2].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')
axes[2].set_xlim([0, 100])
axes[2].grid(axis='x', alpha=0.3)

# Add percentage labels
for i, (val, pct) in enumerate(zip([target_counts[1], target_counts[0]], [target_percent[1], target_percent[0]])):
    axes[2].text(pct + 1, i, f'{val} ({pct:.1f}%)', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 8: Univariate Analysis - Numerical Features
# ============================================================================
"""
Distribution analysis of numerical features
"""

print("=" * 80)
print("UNIVARIATE ANALYSIS - NUMERICAL FEATURES")
print("=" * 80)

numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

fig, axes = plt.subplots(3, 2, figsize=(15, 14))
axes = axes.ravel()

for idx, feature in enumerate(numerical_features):
    # Histogram with KDE
    axes[idx].hist(df[feature], bins=30, alpha=0.6, color='skyblue', edgecolor='black', density=True)

    # Add KDE curve
    from scipy import stats
    kde = stats.gaussian_kde(df[feature].dropna())
    x_range = np.linspace(df[feature].min(), df[feature].max(), 100)
    axes[idx].plot(x_range, kde(x_range), color='red', linewidth=2, label='KDE')

    # Add mean and median lines
    mean_val = df[feature].mean()
    median_val = df[feature].median()

    axes[idx].axvline(mean_val, color='darkred', linestyle='--', linewidth=2,
                     label=f'Mean: {mean_val:.1f}')
    axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=2,
                     label=f'Median: {median_val:.1f}')

    axes[idx].set_title(f'Distribution of {feature}', fontsize=13, fontweight='bold')
    axes[idx].set_xlabel(feature, fontsize=11)
    axes[idx].set_ylabel('Density', fontsize=11)
    axes[idx].legend(fontsize=9)
    axes[idx].grid(axis='y', alpha=0.3)

# Hide extra subplot
axes[-1].axis('off')

plt.suptitle('Numerical Features Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# Statistical tests for normality
print("\nüìä Distribution Statistics:")
print(f"{'Feature':<12} {'Mean':<8} {'Median':<8} {'Std':<8} {'Skewness':<10} {'Kurtosis':<10} {'Shape':<20}")
print("-" * 90)

for feature in numerical_features:
    mean_val = df[feature].mean()
    median_val = df[feature].median()
    std_val = df[feature].std()
    skewness = df[feature].skew()
    kurtosis = df[feature].kurtosis()

    # Determine shape
    if abs(skewness) < 0.5:
        shape = "Fairly Symmetric"
    elif abs(skewness) < 1:
        shape = "Moderately Skewed"
    else:
        shape = "Highly Skewed"

    print(f"{feature:<12} {mean_val:<8.1f} {median_val:<8.1f} {std_val:<8.1f} {skewness:<10.3f} {kurtosis:<10.3f} {shape:<20}")

# ============================================================================
# CELL 9: Univariate Analysis - Categorical Features
# ============================================================================
"""
Distribution analysis of categorical features
"""

print("=" * 80)
print("UNIVARIATE ANALYSIS - CATEGORICAL FEATURES")
print("=" * 80)

categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

fig, axes = plt.subplots(3, 3, figsize=(16, 13))
axes = axes.ravel()

for idx, feature in enumerate(categorical_features):
    if feature in df.columns:
        value_counts = df[feature].value_counts().sort_index()
        colors = plt.cm.Set3(np.linspace(0, 1, len(value_counts)))

        bars = axes[idx].bar(value_counts.index, value_counts.values,
                            color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)

        axes[idx].set_title(f'Distribution of {feature.upper()}', fontsize=12, fontweight='bold')
        axes[idx].set_xlabel(feature, fontsize=11)
        axes[idx].set_ylabel('Count', fontsize=11)
        axes[idx].grid(axis='y', alpha=0.3)

        # Add value labels on bars
        for bar, val in zip(bars, value_counts.values):
            height = bar.get_height()
            pct = val / len(df) * 100
            axes[idx].text(bar.get_x() + bar.get_width()/2., height + 5,
                          f'{val}\n({pct:.1f}%)', ha='center', va='bottom',
                          fontweight='bold', fontsize=9)

# Hide extra subplot
axes[-1].axis('off')

plt.suptitle('Categorical Features Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 10: Bivariate Analysis - Numerical vs Target
# ============================================================================
"""
Analyze relationship between numerical features and target variable
"""

print("=" * 80)
print("BIVARIATE ANALYSIS - NUMERICAL FEATURES vs TARGET")
print("=" * 80)

numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Box plots
fig, axes = plt.subplots(2, 3, figsize=(17, 11))
axes = axes.ravel()

for idx, feature in enumerate(numerical_features):
    sns.boxplot(x='target', y=feature, data=df, ax=axes[idx], palette='Set2')
    axes[idx].set_title(f'{feature.upper()} by Heart Disease Status',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Target (0 = No Disease, 1 = Disease)', fontsize=11)
    axes[idx].set_ylabel(feature, fontsize=11)
    axes[idx].set_xticklabels(['No Disease', 'Disease'])
    axes[idx].grid(axis='y', alpha=0.3)

# Hide extra subplot
axes[-1].axis('off')

plt.suptitle('Box Plots - Numerical Features by Target', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# Violin plots
fig, axes = plt.subplots(2, 3, figsize=(17, 11))
axes = axes.ravel()

for idx, feature in enumerate(numerical_features):
    sns.violinplot(x='target', y=feature, data=df, ax=axes[idx], palette='muted', inner='quartile')
    axes[idx].set_title(f'{feature.upper()} Distribution by Target',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Target', fontsize=11)
    axes[idx].set_ylabel(feature, fontsize=11)
    axes[idx].set_xticklabels(['No Disease', 'Disease'])
    axes[idx].grid(axis='y', alpha=0.3)

axes[-1].axis('off')

plt.suptitle('Violin Plots - Feature Distributions by Target', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# Statistical comparison
print("\nüìä Statistical Comparison by Target:")
print("\nMean Values:")
mean_comparison = df.groupby('target')[numerical_features].mean()
display(mean_comparison.style.background_gradient(cmap='RdYlGn', axis=0))

print("\nMedian Values:")
median_comparison = df.groupby('target')[numerical_features].median()
display(median_comparison.style.background_gradient(cmap='RdYlGn', axis=0))

print("\nStandard Deviation:")
std_comparison = df.groupby('target')[numerical_features].std()
display(std_comparison.style.background_gradient(cmap='YlOrRd', axis=0))

# T-tests for significant differences
from scipy.stats import ttest_ind

print("\nüìä Statistical Significance (t-test):")
print(f"{'Feature':<12} {'t-statistic':<15} {'p-value':<15} {'Significant':<15}")
print("-" * 60)

for feature in numerical_features:
    no_disease = df[df['target'] == 0][feature]
    disease = df[df['target'] == 1][feature]

    t_stat, p_value = ttest_ind(no_disease, disease)
    significant = "Yes (p<0.05)" if p_value < 0.05 else "No (p‚â•0.05)"

    print(f"{feature:<12} {t_stat:<15.4f} {p_value:<15.6f} {significant:<15}")

# Statistical tests for normality
print("\nüìä Skewness Analysis:")
for feature in numerical_features:
    skewness = df[feature].skew()
    print(f"{feature:12s}: {skewness:6.3f} ", end='')
    if abs(skewness) < 0.5:
        print("(Fairly Symmetric)")
    elif abs(skewness) < 1:
        print("(Moderately Skewed)")
    else:
        print("(Highly Skewed)")

# ============================================================================
# CELL 11: Univariate Analysis - Categorical Features
# ============================================================================
"""
Distribution analysis of categorical features
"""

print("=" * 80)
print("UNIVARIATE ANALYSIS - CATEGORICAL FEATURES")
print("=" * 80)

categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

fig, axes = plt.subplots(3, 3, figsize=(16, 12))
axes = axes.ravel()

for idx, feature in enumerate(categorical_features):
    if feature in df.columns:
        value_counts = df[feature].value_counts().sort_index()
        axes[idx].bar(value_counts.index, value_counts.values, color='coral', alpha=0.7, edgecolor='black')
        axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')
        axes[idx].set_xlabel(feature, fontsize=10)
        axes[idx].set_ylabel('Count', fontsize=10)
        axes[idx].grid(axis='y', alpha=0.3)

        # Add value labels on bars
        for i, v in enumerate(value_counts.values):
            axes[idx].text(value_counts.index[i], v + 2, str(v), ha='center', fontweight='bold')

# Hide extra subplot
axes[-1].axis('off')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 12: Bivariate Analysis - Numerical vs Target
# ============================================================================
"""
Analyze relationship between numerical features and target variable
"""

print("=" * 80)
print("BIVARIATE ANALYSIS - NUMERICAL FEATURES vs TARGET")
print("=" * 80)

numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Box plots
fig, axes = plt.subplots(2, 3, figsize=(16, 10))
axes = axes.ravel()

for idx, feature in enumerate(numerical_features):
    sns.boxplot(x='target', y=feature, data=df, ax=axes[idx], palette='Set2')
    axes[idx].set_title(f'{feature} by Heart Disease Status', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Target (0 = No Disease, 1 = Disease)', fontsize=10)
    axes[idx].set_ylabel(feature, fontsize=10)
    axes[idx].set_xticklabels(['No Disease', 'Disease'])

# Hide extra subplot
axes[-1].axis('off')

plt.tight_layout()
plt.show()

# Violin plots
fig, axes = plt.subplots(2, 3, figsize=(16, 10))
axes = axes.ravel()

for idx, feature in enumerate(numerical_features):
    sns.violinplot(x='target', y=feature, data=df, ax=axes[idx], palette='muted')
    axes[idx].set_title(f'{feature} Distribution by Target', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Target', fontsize=10)
    axes[idx].set_ylabel(feature, fontsize=10)
    axes[idx].set_xticklabels(['No Disease', 'Disease'])

axes[-1].axis('off')

plt.tight_layout()
plt.show()

# Statistical comparison
print("\nüìä Mean Values by Target:")
print(df.groupby('target')[numerical_features].mean().round(2))

print("\nüìä Median Values by Target:")
print(df.groupby('target')[numerical_features].median().round(2))

# ============================================================================
# CELL 13: Bivariate Analysis - Categorical vs Target
# ============================================================================
"""
Analyze relationship between categorical features and target variable
"""

print("=" * 80)
print("BIVARIATE ANALYSIS - CATEGORICAL FEATURES vs TARGET")
print("=" * 80)

categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

fig, axes = plt.subplots(3, 3, figsize=(16, 12))
axes = axes.ravel()

for idx, feature in enumerate(categorical_features):
    if feature in df.columns:
        ct = pd.crosstab(df[feature], df['target'], normalize='index') * 100
        ct.plot(kind='bar', ax=axes[idx], color=['lightgreen', 'salmon'], alpha=0.8)
        axes[idx].set_title(f'{feature} vs Target', fontsize=12, fontweight='bold')
        axes[idx].set_xlabel(feature, fontsize=10)
        axes[idx].set_ylabel('Percentage (%)', fontsize=10)
        axes[idx].legend(['No Disease', 'Disease'], loc='best')
        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)

axes[-1].axis('off')

plt.tight_layout()
plt.show()

# Crosstab with counts
print("\nüìä Crosstab Analysis:")
for feature in categorical_features[:3]:  # Show first 3 to save space
    if feature in df.columns:
        print(f"\n{feature}:")
        print(pd.crosstab(df[feature], df['target'], margins=True))

# ============================================================================
# CELL 14: Correlation Analysis
# ============================================================================
"""
Analyze correlations between features
"""

print("=" * 80)
print("CORRELATION ANALYSIS")
print("=" * 80)

# Correlation matrix
correlation_matrix = df.corr()

print("\nüìä Correlation with Target:")
target_corr = correlation_matrix['target'].sort_values(ascending=False)
print(target_corr)

# Heatmap
plt.figure(figsize=(14, 12))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f',
            cmap='coolwarm', center=0, square=True, linewidths=1,
            cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix - Heart Disease Dataset', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# Top correlations with target
print("\nüìä Top Positive Correlations with Target:")
print(target_corr[target_corr > 0.1].drop('target'))

print("\nüìä Top Negative Correlations with Target:")
print(target_corr[target_corr < -0.1])

# ============================================================================
# CELL 15: Pairplot for Key Features
# ============================================================================
"""
Visualize relationships between key features
"""

print("=" * 80)
print("PAIRPLOT - KEY FEATURES")
print("=" * 80)

# Select key features based on correlation with target
key_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'target']

print(f"Creating pairplot for: {', '.join(key_features)}")
print("This may take a moment...")

pairplot = sns.pairplot(df[key_features], hue='target', palette='Set1',
                        diag_kind='kde', plot_kws={'alpha': 0.6},
                        height=2.5)
pairplot.fig.suptitle('Pairplot of Key Features', y=1.02, fontsize=16, fontweight='bold')
plt.show()

print("‚úÖ Pairplot complete!")

# ============================================================================
# CELL 16: Outlier Detection
# ============================================================================
"""
Detect and visualize outliers using IQR method
"""

print("=" * 80)
print("OUTLIER DETECTION")
print("=" * 80)

numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

def detect_outliers_iqr(data, feature):
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[feature] < lower_bound) | (data[feature] > upper_bound)]
    return outliers, lower_bound, upper_bound

print("\nüìä Outlier Summary (IQR Method):")
print(f"{'Feature':<12} {'Outliers':<10} {'Percentage':<12} {'Lower Bound':<15} {'Upper Bound':<15}")
print("-" * 70)

outlier_summary = {}
for feature in numerical_features:
    outliers, lower, upper = detect_outliers_iqr(df, feature)
    outlier_count = len(outliers)
    outlier_pct = (outlier_count / len(df)) * 100
    outlier_summary[feature] = outlier_count
    print(f"{feature:<12} {outlier_count:<10} {outlier_pct:<12.2f}% {lower:<15.2f} {upper:<15.2f}")

# Boxplot for outlier visualization
fig, axes = plt.subplots(1, len(numerical_features), figsize=(18, 5))

for idx, feature in enumerate(numerical_features):
    sns.boxplot(y=df[feature], ax=axes[idx], color='lightblue')
    axes[idx].set_title(f'{feature}', fontsize=12, fontweight='bold')
    axes[idx].set_ylabel('Value', fontsize=10)

    # Add count of outliers
    outlier_count = outlier_summary[feature]
    axes[idx].text(0, df[feature].max(), f'Outliers: {outlier_count}',
                   ha='center', va='bottom', fontweight='bold', color='red')

plt.suptitle('Outlier Detection - Box Plots', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nüí° Note: Outliers are not necessarily errors. In medical data, extreme values may be important!")
print("We will keep outliers as they may represent critical cases.")

# ============================================================================
# CELL 17: Data Preprocessing - Train/Test Split
# ============================================================================
"""
Split data into training and testing sets
"""

print("=" * 80)
print("DATA PREPROCESSING - TRAIN/TEST SPLIT")
print("=" * 80)

# Separate features and target
X = df.drop('target', axis=1)
y = df['target']

print(f"\nüìä Original Dataset:")
print(f"Total samples: {len(df)}")
print(f"Features: {X.shape[1]}")
print(f"Target distribution:\n{y.value_counts()}")

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"\nüìä After Split:")
print(f"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)")
print(f"Testing set:  {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)")

print(f"\nüìä Training Set Target Distribution:")
print(y_train.value_counts())
print(f"Class proportion: {y_train.value_counts(normalize=True).round(3).to_dict()}")

print(f"\nüìä Testing Set Target Distribution:")
print(y_test.value_counts())
print(f"Class proportion: {y_test.value_counts(normalize=True).round(3).to_dict()}")

print("\n‚úÖ Stratified split ensures balanced class distribution in both sets!")

# ============================================================================
# CELL 18: Feature Scaling
# ============================================================================
"""
Standardize features using StandardScaler
"""

print("=" * 80)
print("FEATURE SCALING")
print("=" * 80)

# Initialize scaler
scaler = StandardScaler()

# Fit on training data and transform both sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)

print("‚úÖ Features scaled using StandardScaler")
print("\nüìä Scaling Statistics (from training data):")
print(f"{'Feature':<12} {'Original Mean':<15} {'Original Std':<15} {'Scaled Mean':<15} {'Scaled Std':<15}")
print("-" * 75)

for col in X.columns:
    orig_mean = X_train[col].mean()
    orig_std = X_train[col].std()
    scaled_mean = X_train_scaled[col].mean()
    scaled_std = X_train_scaled[col].std()
    print(f"{col:<12} {orig_mean:<15.2f} {orig_std:<15.2f} {scaled_mean:<15.6f} {scaled_std:<15.6f}")

# Visualize scaling effect
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Before scaling
axes[0].boxplot([X_train[col] for col in X.columns], labels=X.columns)
axes[0].set_title('Before Scaling', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Value', fontsize=12)
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(axis='y', alpha=0.3)

# After scaling
axes[1].boxplot([X_train_scaled[col] for col in X.columns], labels=X.columns)
axes[1].set_title('After Scaling (Standardized)', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Standardized Value', fontsize=12)
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 19: Baseline Model - Dummy Classifier
# ============================================================================
"""
Establish baseline performance using a dummy classifier
"""

print("=" * 80)
print("BASELINE MODEL - DUMMY CLASSIFIER")
print("=" * 80)

from sklearn.dummy import DummyClassifier

# Most frequent strategy
dummy_most_frequent = DummyClassifier(strategy='most_frequent', random_state=42)
dummy_most_frequent.fit(X_train_scaled, y_train)
dummy_pred_mf = dummy_most_frequent.predict(X_test_scaled)

# Stratified strategy
dummy_stratified = DummyClassifier(strategy='stratified', random_state=42)
dummy_stratified.fit(X_train_scaled, y_train)
dummy_pred_strat = dummy_stratified.predict(X_test_scaled)

print("\nüìä Baseline Performance:")
print(f"\nMost Frequent Strategy:")
print(f"  Accuracy: {accuracy_score(y_test, dummy_pred_mf):.4f}")

print(f"\nStratified Strategy:")
print(f"  Accuracy: {accuracy_score(y_test, dummy_pred_strat):.4f}")

baseline_accuracy = accuracy_score(y_test, dummy_pred_mf)
print(f"\nüí° Any model scoring below {baseline_accuracy:.4f} is worse than random guessing!")
print("‚úÖ This is our minimum acceptable performance threshold.")

# ============================================================================
# CELL 20: Model Training - Multiple Algorithms
# ============================================================================
"""
Train multiple classification models and compare performance
"""

print("=" * 80)
print("MODEL TRAINING - MULTIPLE ALGORITHMS")
print("=" * 80)

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'Naive Bayes': GaussianNB()
}

# Store results
results = {}

print("\nüöÄ Training models...\n")

for name, model in models.items():
    print(f"Training {name}...", end=' ')

    # Train model
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else np.nan

    results[name] = {
        'model': model,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc
    }

    print(f"‚úÖ Accuracy: {accuracy:.4f}")

print("\n‚úÖ All models trained successfully!")

# ============================================================================
# CELL 21: Model Comparison - Performance Metrics
# ============================================================================
"""
Compare performance of all trained models
"""

print("=" * 80)
print("MODEL COMPARISON - PERFORMANCE METRICS")
print("=" * 80)

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[m]['accuracy'] for m in results.keys()],
    'Precision': [results[m]['precision'] for m in results.keys()],
    'Recall': [results[m]['recall'] for m in results.keys()],
    'F1-Score': [results[m]['f1_score'] for m in results.keys()],
    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()]
}).sort_values('Accuracy', ascending=False)

print("\nüìä Model Performance Comparison:")
display(comparison_df.style.background_gradient(subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'], cmap='RdYlGn'))

# Visualize comparison
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]

    data = comparison_df.sort_values(metric, ascending=True)
    colors = plt.cm.RdYlGn(data[metric] / data[metric].max())

    ax.barh(data['Model'], data[metric], color=colors)
    ax.set_xlabel(metric, fontsize=12)
    ax.set_title(f'Model Comparison - {metric}', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 1])

    # Add value labels
    for i, v in enumerate(data[metric]):
        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# Best model
best_model_name = comparison_df.iloc[0]['Model']
best_accuracy = comparison_df.iloc[0]['Accuracy']

print(f"\nüèÜ BEST MODEL: {best_model_name}")
print(f"   Accuracy: {best_accuracy:.4f}")
print(f"   Improvement over baseline: {(best_accuracy - baseline_accuracy):.4f} ({(best_accuracy - baseline_accuracy)/baseline_accuracy*100:.1f}%)")

# ============================================================================
# CELL 22: Cross-Validation Analysis
# ============================================================================
"""
Perform k-fold cross-validation to assess model stability
"""

print("=" * 80)
print("CROSS-VALIDATION ANALYSIS")
print("=" * 80)

cv_folds = 5
cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

print(f"\nüîÑ Performing {cv_folds}-Fold Cross-Validation...\n")

cv_results = {}

for name, model in models.items():
    print(f"Evaluating {name}...", end=' ')

    # Perform cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    cv_results[name] = {
        'scores': cv_scores,
        'mean': cv_scores.mean(),
        'std': cv_scores.std()
    }

    print(f"‚úÖ Mean CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")

# Create CV comparison DataFrame
cv_comparison_df = pd.DataFrame({
    'Model': list(cv_results.keys()),
    'CV Mean': [cv_results[m]['mean'] for m in cv_results.keys()],
    'CV Std': [cv_results[m]['std'] for m in cv_results.keys()],
    'Test Accuracy': [results[m]['accuracy'] for m in cv_results.keys()]
}).sort_values('CV Mean', ascending=False)

print("\nüìä Cross-Validation Results:")
display(cv_comparison_df)

# Visualize CV results
plt.figure(figsize=(8, 4))

models_sorted = cv_comparison_df['Model'].values
cv_means = cv_comparison_df['CV Mean'].values
cv_stds = cv_comparison_df['CV Std'].values

y_pos = np.arange(len(models_sorted))

plt.barh(y_pos, cv_means, xerr=cv_stds, alpha=0.7, color='skyblue',
         edgecolor='black', error_kw={'elinewidth': 2, 'ecolor': 'red'})
plt.yticks(y_pos, models_sorted)
plt.xlabel('Cross-Validation Accuracy', fontsize=12)
plt.title(f'{cv_folds}-Fold Cross-Validation Results', fontsize=14, fontweight='bold')
plt.axvline(x=baseline_accuracy, color='red', linestyle='--', linewidth=2, label='Baseline')
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

# Check for overfitting
print("\n‚ö†Ô∏è Overfitting Check (CV Score vs Test Score):")
for name in cv_results.keys():
    cv_score = cv_results[name]['mean']
    test_score = results[name]['accuracy']
    diff = cv_score - test_score

    print(f"{name:25s}: CV={cv_score:.4f}, Test={test_score:.4f}, Diff={diff:+.4f}", end='')

    if abs(diff) > 0.05:
        print(" ‚ö†Ô∏è Potential overfitting/underfitting")
    else:
        print(" ‚úÖ Good generalization")

# ============================================================================
# CELL 24: ROC Curves
# ============================================================================
"""
Plot ROC curves for models with probability predictions
"""

print("=" * 80)
print("ROC CURVES ANALYSIS")
print("=" * 80)

plt.figure(figsize=(8, 6))

# Plot ROC curve for each model
for name, result in results.items():
    if result['y_pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
        auc = result['roc_auc']
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)

# Plot diagonal (random classifier)
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)', linewidth=2)

plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Find best model by ROC-AUC
roc_auc_scores = {name: result['roc_auc'] for name, result in results.items() if not np.isnan(result['roc_auc'])}
best_roc_model = max(roc_auc_scores, key=roc_auc_scores.get)

print(f"\nüèÜ Best Model by ROC-AUC: {best_roc_model} (AUC = {roc_auc_scores[best_roc_model]:.4f})")

# ============================================================================
# CELL 25: Feature Importance - Random Forest
# ============================================================================
"""
Analyze feature importance from Random Forest model
"""

print("=" * 80)
print("FEATURE IMPORTANCE ANALYSIS - RANDOM FOREST")
print("=" * 80)

# Get Random Forest model
rf_model = results['Random Forest']['model']

# Extract feature importances
feature_importance_rf = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nüìä Feature Importance (Random Forest):")
display(feature_importance_rf)

# Visualize feature importance
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar plot
axes[0].barh(feature_importance_rf['Feature'], feature_importance_rf['Importance'],
            color='teal', alpha=0.7, edgecolor='black')
axes[0].set_xlabel('Importance', fontsize=12)
axes[0].set_title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')
axes[0].invert_yaxis()
axes[0].grid(axis='x', alpha=0.3)

# Add percentage labels
for i, v in enumerate(feature_importance_rf['Importance']):
    axes[0].text(v + 0.001, i, f'{v:.3f}', va='center', fontweight='bold')

# Pie chart for top features
top_n = 5
top_features = feature_importance_rf.head(top_n)
other_importance = feature_importance_rf.iloc[top_n:]['Importance'].sum()

pie_data = list(top_features['Importance']) + [other_importance]
pie_labels = list(top_features['Feature']) + ['Others']

axes[1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', startangle=90,
           colors=sns.color_palette('Set3', len(pie_labels)))
axes[1].set_title(f'Top {top_n} Features Contribution', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print(f"\nüí° Top 3 Most Important Features:")
for i, row in enumerate(feature_importance_rf.head(3).iterrows(), 1):
    idx, data = row
    print(f"  {i}. {data['Feature']}: {data['Importance']:.4f}")

# ============================================================================
# CELL 26: Hyperparameter Tuning - Random Forest
# ============================================================================
"""
Optimize Random Forest hyperparameters using GridSearchCV
"""

print("=" * 80)
print("HYPERPARAMETER TUNING - RANDOM FOREST")
print("=" * 80)

# Define parameter grid
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

print(f"\nüîç Parameter Grid:")
for param, values in param_grid_rf.items():
    print(f"  {param}: {values}")

total_combinations = np.prod([len(v) for v in param_grid_rf.values()])
print(f"\nTotal combinations to test: {total_combinations}")

# Initialize GridSearchCV
grid_search_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("\nüöÄ Starting Grid Search for Random Forest... (this may take several minutes)")

# Fit grid search
grid_search_rf.fit(X_train_scaled, y_train)

print("\n‚úÖ Grid Search Complete!")

# Best parameters
print(f"\nüèÜ Best Parameters:")
for param, value in grid_search_rf.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nüìä Best Cross-Validation Score: {grid_search_rf.best_score_:.4f}")

# Test on test set
best_rf_model = grid_search_rf.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test_scaled)
y_pred_proba_best_rf = best_rf_model.predict_proba(X_test_scaled)[:, 1]
best_rf_accuracy = accuracy_score(y_test, y_pred_best_rf)
best_rf_roc_auc = roc_auc_score(y_test, y_pred_proba_best_rf)

print(f"üìä Test Set Accuracy: {best_rf_accuracy:.4f}")
print(f"üìä Test Set ROC-AUC: {best_rf_roc_auc:.4f}")
print(f"üìä Improvement over default RF: {best_rf_accuracy - results['Random Forest']['accuracy']:.4f}")

# Visualize grid search results (top 10 combinations)
cv_results_df = pd.DataFrame(grid_search_rf.cv_results_)
# Set option to show full content
with pd.option_context('display.max_colwidth', None):
    top_results = cv_results_df.nlargest(10, 'mean_test_score')[
        ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    ]
    print("\nüìä Top 10 Parameter Combinations:")
    display(top_results)

# ============================================================================
# CELL 27: Hyperparameter Tuning - XGBoost
# ============================================================================
"""
Optimize XGBoost hyperparameters using GridSearchCV
"""

print("=" * 80)
print("HYPERPARAMETER TUNING - XGBOOST")
print("=" * 80)

# Define parameter grid
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

print(f"\nüîç Parameter Grid:")
for param, values in param_grid_xgb.items():
    print(f"  {param}: {values}")

total_combinations = np.prod([len(v) for v in param_grid_xgb.values()])
print(f"\nTotal combinations to test: {total_combinations}")

# Initialize GridSearchCV
grid_search_xgb = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
    param_grid_xgb,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("\nüöÄ Starting Grid Search for XGBoost... (this may take several minutes)")

# Fit grid search
grid_search_xgb.fit(X_train_scaled, y_train)

print("\n‚úÖ Grid Search Complete!")

# Best parameters
print(f"\nüèÜ Best Parameters:")
for param, value in grid_search_xgb.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nüìä Best Cross-Validation Score: {grid_search_xgb.best_score_:.4f}")

# Test on test set
best_xgb_model = grid_search_xgb.best_estimator_
y_pred_best_xgb = best_xgb_model.predict(X_test_scaled)
y_pred_proba_best_xgb = best_xgb_model.predict_proba(X_test_scaled)[:, 1]
best_xgb_accuracy = accuracy_score(y_test, y_pred_best_xgb)
best_xgb_roc_auc = roc_auc_score(y_test, y_pred_proba_best_xgb)

print(f"üìä Test Set Accuracy: {best_xgb_accuracy:.4f}")
print(f"üìä Test Set ROC-AUC: {best_xgb_roc_auc:.4f}")
print(f"üìä Improvement over default XGBoost: {best_xgb_accuracy - results['XGBoost']['accuracy']:.4f}")

# Visualize grid search results with NO TRUNCATION
cv_results_df_xgb = pd.DataFrame(grid_search_xgb.cv_results_)

print("\nüìä Top 10 Parameter Combinations:")
with pd.option_context('display.max_colwidth', None):
    top_results_xgb = cv_results_df_xgb.nlargest(10, 'mean_test_score')[
        ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
    ]
    display(top_results_xgb)

# Compare tuned models
print("\nüìä Comparison: Default vs Tuned Models")
comparison_tuning = pd.DataFrame({
    'Model': ['RF (Default)', 'RF (Tuned)', 'XGBoost (Default)', 'XGBoost (Tuned)'],
    'Test Accuracy': [
        results['Random Forest']['accuracy'],
        best_rf_accuracy,
        results['XGBoost']['accuracy'],
        best_xgb_accuracy
    ],
    'ROC-AUC': [
        results['Random Forest']['roc_auc'],
        best_rf_roc_auc,
        results['XGBoost']['roc_auc'],
        best_xgb_roc_auc
    ]
})

display(comparison_tuning)

# Visualize improvement
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy comparison
axes[0].bar(['RF\nDefault', 'RF\nTuned', 'XGB\nDefault', 'XGB\nTuned'],
           comparison_tuning['Test Accuracy'],
           color=['lightblue', 'darkblue', 'lightgreen', 'darkgreen'],
           alpha=0.7, edgecolor='black')
axes[0].set_ylabel('Test Accuracy', fontsize=12)
axes[0].set_title('Accuracy: Default vs Tuned', fontsize=14, fontweight='bold')
axes[0].set_ylim([0.7, 1.0])
axes[0].grid(axis='y', alpha=0.3)

for i, v in enumerate(comparison_tuning['Test Accuracy']):
    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')

# ROC-AUC comparison
axes[1].bar(['RF\nDefault', 'RF\nTuned', 'XGB\nDefault', 'XGB\nTuned'],
           comparison_tuning['ROC-AUC'],
           color=['lightblue', 'darkblue', 'lightgreen', 'darkgreen'],
           alpha=0.7, edgecolor='black')
axes[1].set_ylabel('ROC-AUC', fontsize=12)
axes[1].set_title('ROC-AUC: Default vs Tuned', fontsize=14, fontweight='bold')
axes[1].set_ylim([0.7, 1.0])
axes[1].grid(axis='y', alpha=0.3)

for i, v in enumerate(comparison_tuning['ROC-AUC']):
    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 28: Final Model Selection
# ============================================================================
"""
Select the best model based on all evaluations
"""

print("=" * 80)
print("FINAL MODEL SELECTION")
print("=" * 80)

# Compare all models including tuned versions
final_comparison = pd.DataFrame({
    'Model': list(results.keys()) + ['RF (Tuned)', 'XGBoost (Tuned)'],
    'Test Accuracy': [results[m]['accuracy'] for m in results.keys()] +
                     [best_rf_accuracy, best_xgb_accuracy],
    'F1-Score': [results[m]['f1_score'] for m in results.keys()] +
                [f1_score(y_test, y_pred_best_rf), f1_score(y_test, y_pred_best_xgb)],
    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()] +
               [best_rf_roc_auc, best_xgb_roc_auc]
}).sort_values('Test Accuracy', ascending=False)

print("\nüìä Final Model Comparison (Including Tuned Models):")
display(final_comparison.style.background_gradient(subset=['Test Accuracy', 'F1-Score', 'ROC-AUC'], cmap='RdYlGn'))

# Select best model
best_final_model_name = final_comparison.iloc[0]['Model']
best_final_accuracy = final_comparison.iloc[0]['Test Accuracy']

print(f"\nüèÜ FINAL BEST MODEL: {best_final_model_name}")
print(f"   Test Accuracy: {best_final_accuracy:.4f}")
print(f"   F1-Score: {final_comparison.iloc[0]['F1-Score']:.4f}")
print(f"   ROC-AUC: {final_comparison.iloc[0]['ROC-AUC']:.4f}")

# Determine which model object to use
if best_final_model_name == 'RF (Tuned)':
    final_best_model = best_rf_model
    y_pred_final = y_pred_best_rf
    y_pred_proba_final = y_pred_proba_best_rf
elif best_final_model_name == 'XGBoost (Tuned)':
    final_best_model = best_xgb_model
    y_pred_final = y_pred_best_xgb
    y_pred_proba_final = y_pred_proba_best_xgb
else:
    final_best_model = results[best_final_model_name]['model']
    y_pred_final = results[best_final_model_name]['y_pred']
    y_pred_proba_final = results[best_final_model_name]['y_pred_proba']

# ============================================================================
# CELL 29: Final Model Evaluation
# ============================================================================
"""
Comprehensive evaluation of the final best model
"""

print("=" * 80)
print(f"FINAL MODEL EVALUATION - {best_final_model_name}")
print("=" * 80)

# Calculate all metrics
final_metrics = {
    'Accuracy': accuracy_score(y_test, y_pred_final),
    'Precision': precision_score(y_test, y_pred_final),
    'Recall': recall_score(y_test, y_pred_final),
    'F1-Score': f1_score(y_test, y_pred_final),
    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_final),
    'Average Precision': average_precision_score(y_test, y_pred_proba_final)
}

print("\nüìä Final Model Metrics:")
for metric, value in final_metrics.items():
    print(f"  {metric:<20}: {value:.4f}")

# Confusion matrix
cm_final = confusion_matrix(y_test, y_pred_final)
tn, fp, fn, tp = cm_final.ravel()

print(f"\nüìä Confusion Matrix:")
print(f"  True Negatives:  {tn}")
print(f"  False Positives: {fp}")
print(f"  False Negatives: {fn}")
print(f"  True Positives:  {tp}")

print(f"\nüìä Additional Metrics:")
print(f"  Specificity: {tn/(tn+fp):.4f}")
print(f"  Sensitivity: {tp/(tp+fn):.4f}")
print(f"  NPV (Negative Predictive Value): {tn/(tn+fn) if (tn+fn) > 0 else 0:.4f}")
print(f"  PPV (Positive Predictive Value): {tp/(tp+fp) if (tp+fp) > 0 else 0:.4f}")

# Visualize final confusion matrix
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Confusion matrix heatmap
sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=axes[0],
           square=True, linewidths=2, linecolor='black',
           xticklabels=['No Disease', 'Disease'],
           yticklabels=['No Disease', 'Disease'])
axes[0].set_title(f'Confusion Matrix - {best_final_model_name}', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Predicted', fontsize=12)
axes[0].set_ylabel('Actual', fontsize=12)

# Normalized confusion matrix
cm_normalized = cm_final.astype('float') / cm_final.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],
           square=True, linewidths=2, linecolor='black',
           xticklabels=['No Disease', 'Disease'],
           yticklabels=['No Disease', 'Disease'])
axes[1].set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Predicted', fontsize=12)
axes[1].set_ylabel('Actual', fontsize=12)

plt.tight_layout()
plt.show()

# Classification report
print("\nüìã Detailed Classification Report:")
print(classification_report(y_test, y_pred_final, target_names=['No Disease', 'Disease']))

# ============================================================================
# CELL 30: Save Final Model
# ============================================================================
"""
Save the trained model and scaler for deployment
"""

print("=" * 80)
print("SAVING MODEL ARTIFACTS")
print("=" * 80)

import os

# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)

# Save the best model
model_filename = 'models/heart_disease_model.pkl'
joblib.dump(final_best_model, model_filename)
print(f"‚úÖ Model saved: {model_filename}")

# Save the scaler
scaler_filename = 'models/scaler.pkl'
joblib.dump(scaler, scaler_filename)
print(f"‚úÖ Scaler saved: {scaler_filename}")

# Save feature names
feature_names_filename = 'models/feature_names.pkl'
joblib.dump(list(X.columns), feature_names_filename)
print(f"‚úÖ Feature names saved: {feature_names_filename}")

# Save model metadata
metadata = {
    'model_name': best_final_model_name,
    'model_type': type(final_best_model).__name__,
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
    'test_accuracy': float(best_final_accuracy),
    'test_f1_score': float(final_comparison.iloc[0]['F1-Score']),
    'test_roc_auc': float(final_comparison.iloc[0]['ROC-AUC']),
    'n_features': X.shape[1],
    'feature_names': list(X.columns),
    'training_samples': len(X_train),
    'test_samples': len(X_test),
    'dataset_size': len(df),
    'metrics': final_metrics
}

metadata_filename = 'models/model_metadata.pkl'
joblib.dump(metadata, metadata_filename)
print(f"‚úÖ Model metadata saved: {metadata_filename}")

print("\nüì¶ All model artifacts saved successfully!")
print(f"\nüìÅ Saved files:")
print(f"  ‚Ä¢ {model_filename}")
print(f"  ‚Ä¢ {scaler_filename}")
print(f"  ‚Ä¢ {feature_names_filename}")
print(f"  ‚Ä¢ {metadata_filename}")

# Verify saved model works
print("\nüîç Verifying saved model...")
loaded_model = joblib.load(model_filename)
loaded_scaler = joblib.load(scaler_filename)

# Test prediction
test_prediction = loaded_model.predict(loaded_scaler.transform(X_test.iloc[:1]))
print(f"‚úÖ Model loaded and tested successfully!")
print(f"   Test prediction: {test_prediction[0]}")

# ============================================================================
# CELL 31: Final Summary
# ============================================================================
"""
Final summary of model performance and key findings
"""

print("=" * 80)
print("PROJECT SUMMARY")
print("=" * 80)

print("\n" + "="*80)
print("HEART DISEASE PREDICTION - PROJECT SUMMARY")
print("="*80)

print(f"\nüìä DATASET:")
print(f"  ‚Ä¢ Total samples: {len(df)}")
print(f"  ‚Ä¢ Features: {X.shape[1]}")
print(f"  ‚Ä¢ Target distribution: {dict(y.value_counts())}")
print(f"  ‚Ä¢ Class balance: {dict((y.value_counts() / len(y) * 100).round(1))}")

print(f"\nüîç DATA QUALITY:")
print(f"  ‚Ä¢ Missing values: 0")
print(f"  ‚Ä¢ Duplicate rows: 0")
print(f"  ‚Ä¢ Outliers detected: Yes (kept for medical relevance)")

print(f"\nü§ñ MODELS EVALUATED:")
model_count = len(models) + 2  # Original models + 2 tuned versions
print(f"  ‚Ä¢ Total models tested: {model_count}")
print(f"  ‚Ä¢ Baseline accuracy: {baseline_accuracy:.4f}")
print(f"  ‚Ä¢ Best model: {best_final_model_name}")
print(f"  ‚Ä¢ Best accuracy: {best_final_accuracy:.4f}")
print(f"  ‚Ä¢ Improvement over baseline: {(best_final_accuracy - baseline_accuracy):.4f} ({(best_final_accuracy - baseline_accuracy)/baseline_accuracy*100:.1f}%)")

print(f"\nüìà BEST MODEL PERFORMANCE:")
print(f"  ‚Ä¢ Accuracy: {final_metrics['Accuracy']:.4f}")
print(f"  ‚Ä¢ Precision: {final_metrics['Precision']:.4f}")
print(f"  ‚Ä¢ Recall: {final_metrics['Recall']:.4f}")
print(f"  ‚Ä¢ F1-Score: {final_metrics['F1-Score']:.4f}")
print(f"  ‚Ä¢ ROC-AUC: {final_metrics['ROC-AUC']:.4f}")

print(f"\nüéØ KEY FINDINGS:")
if hasattr(final_best_model, 'feature_importances_'):
    importances = pd.Series(final_best_model.feature_importances_, index=X.columns).sort_values(ascending=False)
    print(f"  ‚Ä¢ Top 3 important features:")
    for i, (feat, imp) in enumerate(importances.head(3).items(), 1):
        print(f"    {i}. {feat}: {imp:.4f}")

print(f"\nüíæ DELIVERABLES:")
print(f"  ‚Ä¢ Trained model saved")
print(f"  ‚Ä¢ Feature scaler saved")
print(f"  ‚Ä¢ Model metadata saved")
print(f"  ‚Ä¢ Ready for deployment")

print(f"\nüöÄ NEXT STEPS:")
print(f"  ‚Ä¢ Deploy model as REST API (Flask/FastAPI)")
print(f"  ‚Ä¢ Create web interface for doctors")
print(f"  ‚Ä¢ Implement monitoring and retraining pipeline")

print("\n" + "="*80)
print("‚úÖ PROJECT COMPLETED SUCCESSFULLY!")
print("="*80)

